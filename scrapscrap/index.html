<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>scrap scrap scrap scrap scrap</title>

    <style>
        @font-face {
            font-family: "Terminal Grotesque Open";
            src: url('./fonts/terminal-grotesque_open.otf');
        }

        @font-face {
            font-family: "Terminal Grotesque ";
            src: url('./fonts/terminal-grotesque.ttf');
        }

        body {
            width: 50%;
            margin: auto
        }

        h1 {
            font-family: "Terminal Grotesque Open";
            font-size: 2em;
            text-align: center;
        }

        pre {
            padding: 1em;
            border: 1px solid black
        }

        button {
            font-family: "Terminal Grotesque";
            font-size: 1.2em
        }

        .pick {
            position: absolute;
            animation: scroll 30s linear;
            font-size: 2em;
            top: 0;
        }

        @keyframes scroll {
            0% {
                transform: translateY(0vh) rotate(-10deg);
            }

            25% {
                transform: translateY(25vh) rotate(-20deg);
            }

            50% {
                transform: translateY(50vh) rotate(0deg);
            }

            75% {
                transform: translateY(75vh) rotate(-20deg);
            }

            100% {
                transform: translateY(100vh) rotate(-10deg);
            }
        }
    </style>
</head>

<body>
    <h1>scrap, scrap... You are the web crawler!</h1>

    <button style="float:right; margin-bottom: 3em"><a
            href="https://link.excalidraw.com/l/8fFb2rptKCY/GNvEfhWpAW">Excalidraw board</a></button>
    <button id="copyBtn">Copy</button>
    <pre id="code"><code>
# LET'S START DIGGING
# use the requests library to request a copy of the HTML document
import requests

website = "http://mouchette.org/"

r = requests.get(website)
print(r.text)

# COLLECT DIRT IN THE BUCKET
# download the page into a dump folder using the os library
import os

with open("dump/index.html", "w") as f:
    f.write(r.text)

# WE GOT SOMETHING
# use the bs4 library to find only the elements containing href and src

from bs4 import BeautifulSoup

soup = BeautifulSoup(r.text, "html.parser")

hrefs = [tag["href"] for tag in soup.find_all(href=True)]
print("hrefs", hrefs)

srcs  = [tag["src"]  for tag in soup.find_all(src=True)]
print("srcs:", srcs)
    
# CLEAN IT UP FROM THE DIRT
# we can remove duplicate links by creating a list with all the href and src
links = hrefs + srcs
links = list(dict.fromkeys(links))
print("unique links:", links)

# RINSE IT WITH WATER
# let's filter only pages that are HTML and place the media aside
pages = [p for p in links if ".html" in p or ".php" in p]
print(pages)

media = [m for m in links if ".jpg" in m or ".gif" in m]
print(media)

# LABEL IT
# let's transform the text into links (we add the "/name_of_page.html" to "mouchette.org")

from urllib.parse import urljoin
from pathvalidate import sanitize_filepath

urls = pages + media
urls = [urljoin(website, u) for u in urls]
print(urls)

# BRING IT HOME
# let's download the content we found inside the dump folder

for u in urls:
    data = requests.get(u).content
    
    u = u.replace(website, "")
    
    path = "dump/" + u
    path = sanitize_filepath("dump/" + u)
    
    os.makedirs(os.path.dirname(path), exist_ok=True)
        
    with open(path, "wb") as f:
        f.write(data)
        
        print(u)

# DIG DEEPER
# repeat the process for all discovered HTML pages

# visited = set()
queue = [website]

while queue:

    url = queue.pop(0)

    print("DIGGING:", url)

    r = requests.get(url)
    content = r.text

    soup = BeautifulSoup(content, "html.parser")

    hrefs = [tag["href"] for tag in soup.find_all(href=True)]
    srcs  = [tag["src"]  for tag in soup.find_all(src=True)]
    links = list(dict.fromkeys(hrefs + srcs))

    # separate pages and media
    pages = [l for l in links if ".html" in l or ".php" in l]
    media = [l for l in links if ".jpg" in l or ".gif" in l]

    # make absolute
    pages = [urljoin(url, p) for p in pages]
    media = [urljoin(url, m) for m in media]
    
    from urllib.parse import urlparse

    # download media
    for m in media:
        data = requests.get(m).content
        path = urlparse(m).path.lstrip("/")
        local = "dump/" + path

        os.makedirs(os.path.dirname(local), exist_ok=True)

        with open(local, "wb") as f:
            f.write(data)
            print("downloaded:", path)

    # queue new pages
    for p in pages:
        if "mouchette.org" in p:
            queue.append(p)
    </code></pre>


    <script>
        document.getElementById("copyBtn").addEventListener("click", () => {
            const text = document.getElementById("code").innerText;

            navigator.clipboard.writeText(text)
                .then(() => alert("AAaaAaaAAh! you copied the text. now head back to vscodium and paste it :)"))
                .catch(err => console.error("Copy failed:", err));
        });


        setInterval(() => {
            setTimeout(() => {
                const pick = document.createElement("span");
                pick.innerHTML = "⛏️";
                pick.classList.add("pick")
                pick.style.left = `${Math.random() * 90}vw`
                document.body.appendChild(pick);
                pick.addEventListener("click", () => { pick.remove() })
            }, 10000);
        }, 10000)
    </script>

</body>

</html>